{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge: Evaluating AI Model Outputs with LLMs\n",
    "\n",
    "## The True Purpose of LLM-as-a-Judge\n",
    "\n",
    "**Important Distinction:** LLM-as-a-Judge is NOT about using an LLM to classify text. That's just an LLM classifier.\n",
    "\n",
    "**LLM-as-a-Judge** is about using an LLM to **evaluate whether an existing AI model's output is correct, accurate, or high-quality**.\n",
    "\n",
    "### The Real Use Case\n",
    "\n",
    "Imagine you have:\n",
    "- A classification model that predicts sentiment (Positive/Negative/Neutral)\n",
    "- A chatbot that answers customer questions\n",
    "- An AI that generates product descriptions\n",
    "- A model that extracts entities from text\n",
    "\n",
    "**How do you know if these outputs are good?**\n",
    "\n",
    "Traditional approaches:\n",
    "- Manual human evaluation (expensive, slow)\n",
    "- Comparing to ground truth labels (requires labeled data)\n",
    "- Simple metrics (accuracy, F1) - don't capture nuance\n",
    "\n",
    "**LLM-as-a-Judge approach:**\n",
    "- Use a powerful LLM to evaluate each prediction\n",
    "- Get reasoning for why an output is good or bad\n",
    "- Score outputs on multiple dimensions\n",
    "- Enable quality control at scale\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "In this tutorial, we'll build a production-ready LLM-as-a-Judge framework that:\n",
    "\n",
    "1. **Evaluates AI Outputs** - Not classifies, but judges existing classifications\n",
    "2. **Uses Few-Shot Examples** - Shows the judge what good vs. bad outputs look like\n",
    "3. **Provides ReAct-style Reasoning** - Confidence scores and chain-of-thought explanations\n",
    "4. **Returns Structured Output** - Pydantic models for production systems\n",
    "5. **Works with Any LLM** - OpenAI, Anthropic, or custom models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install dependencies and import our framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install openai pydantic pandas\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Import our LLM-as-a-Judge framework\n",
    "from llm_judge import LLMJudge, FewShotExample, JudgmentResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Task - Judging AI Outputs\n",
    "\n",
    "### Example: Sentiment Classification Model\n",
    "\n",
    "Let's say we have a sentiment classification model that we want to evaluate. The model takes customer reviews and predicts: Positive, Negative, or Neutral.\n",
    "\n",
    "Here's sample data our model classified:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Model Predictions to Evaluate:\n",
      "                                               input model_output ground_truth\n",
      "0  This product is absolutely amazing! Best purch...     Positive     Positive\n",
      "1  The item arrived damaged and customer service ...     Negative     Negative\n",
      "2  It's okay, nothing special but does the job I ...     Positive      Neutral\n",
      "3  Terrible quality. Broke after one use. Very di...      Neutral     Negative\n"
     ]
    }
   ],
   "source": [
    "# Sample outputs from our AI sentiment classifier\n",
    "model_predictions = [\n",
    "    {\n",
    "        \"input\": \"This product is absolutely amazing! Best purchase ever!\",\n",
    "        \"model_output\": \"Positive\",\n",
    "        \"ground_truth\": \"Positive\"  # We'll use this later\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"The item arrived damaged and customer service was unhelpful.\",\n",
    "        \"model_output\": \"Negative\",\n",
    "        \"ground_truth\": \"Negative\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"It's okay, nothing special but does the job I guess.\",\n",
    "        \"model_output\": \"Positive\",  # This looks wrong!\n",
    "        \"ground_truth\": \"Neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Terrible quality. Broke after one use. Very disappointed.\",\n",
    "        \"model_output\": \"Neutral\",  # This also looks wrong!\n",
    "        \"ground_truth\": \"Negative\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(model_predictions)\n",
    "print(\"AI Model Predictions to Evaluate:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Question\n",
    "\n",
    "For each prediction, we want to know:\n",
    "- **Is this output correct?**\n",
    "- **How confident are we in that judgment?**\n",
    "- **Why is it correct or incorrect?**\n",
    "- **How would we score the quality?**\n",
    "\n",
    "This is where LLM-as-a-Judge comes in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Role and Task Definition\n",
    "\n",
    "Just like with any prompt engineering, we need to clearly define:\n",
    "1. **Who is the judge?** - Their expertise and background\n",
    "2. **What are they evaluating?** - The specific task\n",
    "3. **What criteria should they use?** - How to determine if an output is good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the judge's role\n",
    "judge_role = \"\"\"You are an expert evaluator of sentiment classification models.\n",
    "You have 10 years of experience in natural language processing and understand the nuances\n",
    "of sentiment analysis. Your job is to assess whether an AI model's sentiment predictions\n",
    "are accurate and appropriate given the input text.\"\"\"\n",
    "\n",
    "# Define the evaluation task\n",
    "evaluation_task = \"\"\"For each AI model prediction, you must:\n",
    "1. Read the original customer review\n",
    "2. Examine the AI model's predicted sentiment\n",
    "3. Determine if the prediction is correct\n",
    "4. Consider nuances like sarcasm, mixed sentiment, and intensity\n",
    "5. Provide a quality score (0-100), verdict, and detailed reasoning\"\"\"\n",
    "\n",
    "# Define evaluation criteria\n",
    "evaluation_criteria = \"\"\"\n",
    "A sentiment prediction is correct if:\n",
    "- Positive: The review expresses satisfaction, praise, or positive experiences\n",
    "- Negative: The review expresses dissatisfaction, complaints, or negative experiences\n",
    "- Neutral: The review is factual, balanced, or lacks strong sentiment\n",
    "\n",
    "Consider:\n",
    "- Intensity of language (e.g., \"amazing\" vs \"okay\")\n",
    "- Context and implications\n",
    "- Mixed sentiments (judge the dominant sentiment)\n",
    "\"\"\"\n",
    "\n",
    "# Valid verdicts\n",
    "verdicts = [\"Correct\", \"Incorrect\", \"Partially Correct\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Few-Shot Examples - Teaching the Judge\n",
    "\n",
    "Few-shot examples are crucial for LLM-as-a-Judge. They show:\n",
    "- What a correct prediction looks like\n",
    "- What an incorrect prediction looks like\n",
    "- How to reason about edge cases\n",
    "- How to calibrate scores and confidence\n",
    "\n",
    "Let's create examples that demonstrate good judgment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 few-shot examples to guide the judge\n"
     ]
    }
   ],
   "source": [
    "# Create few-shot examples showing how to judge outputs\n",
    "# NOTE: We do NOT include ground_truth in the examples - the judge must evaluate\n",
    "# based solely on the input and model output, as it would in real production scenarios\n",
    "examples = [\n",
    "    FewShotExample(\n",
    "        input_text=\"This product exceeded all my expectations! Highly recommend!\",\n",
    "        model_output=\"Positive\",\n",
    "        expected_verdict=\"Correct\",\n",
    "        expected_score=100.0,\n",
    "        reasoning=\"\"\"The model correctly identified this as Positive sentiment. \n",
    "        The review contains strong positive indicators: 'exceeded expectations' and \n",
    "        'highly recommend'. The prediction is accurate and appropriate.\"\"\"\n",
    "    ),\n",
    "    FewShotExample(\n",
    "        input_text=\"Worst purchase of my life. Complete waste of money.\",\n",
    "        model_output=\"Negative\",\n",
    "        expected_verdict=\"Correct\",\n",
    "        expected_score=100.0,\n",
    "        reasoning=\"\"\"The model correctly identified this as Negative sentiment.\n",
    "        The review contains strong negative indicators: 'worst' and 'waste of money'.\n",
    "        The prediction is accurate.\"\"\"\n",
    "    ),\n",
    "    FewShotExample(\n",
    "        input_text=\"It's fine, does what it says on the box.\",\n",
    "        model_output=\"Positive\",\n",
    "        expected_verdict=\"Incorrect\",\n",
    "        expected_score=20.0,\n",
    "        reasoning=\"\"\"The model incorrectly classified this as Positive when it should be Neutral.\n",
    "        The review is lukewarm at best - 'fine' and 'does what it says' indicate no strong sentiment.\n",
    "        This is a clear misclassification that could skew model performance metrics.\"\"\"\n",
    "    ),\n",
    "    FewShotExample(\n",
    "        input_text=\"Great features but terrible battery life really ruins it.\",\n",
    "        model_output=\"Neutral\",\n",
    "        expected_verdict=\"Partially Correct\",\n",
    "        expected_score=60.0,\n",
    "        reasoning=\"\"\"The model classified this as Neutral, which is defensible but not ideal.\n",
    "        The review has mixed sentiment: positive ('great features') and negative ('terrible battery').\n",
    "        However, the word 'ruins' suggests the negative aspect dominates. A Negative classification\n",
    "        would be more accurate, but Neutral shows the model recognized the mixed nature.\"\"\"\n",
    "    ),\n",
    "    FewShotExample(\n",
    "        input_text=\"Meh. Nothing to write home about.\",\n",
    "        model_output=\"Negative\",\n",
    "        expected_verdict=\"Incorrect\",\n",
    "        expected_score=30.0,\n",
    "        reasoning=\"\"\"The model incorrectly classified this as Negative when it should be Neutral.\n",
    "        'Meh' and 'nothing to write home about' indicate indifference, not negativity.\n",
    "        While slightly negative-leaning, this is more neutral disappointment than active dissatisfaction.\"\"\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(examples)} few-shot examples to guide the judge\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway on Few-Shot Examples\n",
    "\n",
    "Notice how our examples:\n",
    "- Cover different scenarios: correct, incorrect, and partially correct\n",
    "- Show score calibration (100 for perfect, 20-30 for clear errors, 60 for debatable cases)\n",
    "- Explain the reasoning in detail\n",
    "- Reference specific words/phrases from the input\n",
    "- **Do NOT include ground truth** - the judge learns to evaluate quality intrinsically\n",
    "\n",
    "This teaches the judge how to think about evaluations based solely on input quality, not by comparison to a known answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Creating the Judge\n",
    "\n",
    "Now let's instantiate our LLM judge with all the components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge created successfully!\n",
      "\n",
      "System Prompt Preview (first 500 chars):\n",
      "# ROLE\n",
      "You are an expert evaluator of sentiment classification models.\n",
      "You have 10 years of experience in natural language processing and understand the nuances\n",
      "of sentiment analysis. Your job is to assess whether an AI model's sentiment predictions\n",
      "are accurate and appropriate given the input text.\n",
      "\n",
      "# TASK\n",
      "For each AI model prediction, you must:\n",
      "1. Read the original customer review\n",
      "2. Examine the AI model's predicted sentiment\n",
      "3. Determine if the prediction is correct\n",
      "4. Consider nuances like sarcasm, mixed sentiment, and intensity\n",
      "5. Provide a quality score (0-100), verdict, and detailed reasoning\n",
      "\n",
      "# EVALUATION CRITERIA\n",
      "\n",
      "A sentiment prediction is correct if:\n",
      "- Positive: The review expresses satisfaction, praise, or positive experiences\n",
      "- Negative: The review expresses dissatisfaction, complaints, or negative experiences\n",
      "- Neutral: The review is factual, balanced, or lacks strong sentiment\n",
      "\n",
      "Consider:\n",
      "- Intensity of language (e.g., \"amazing\" vs \"okay\")\n",
      "- Context and implications\n",
      "- Mixed sentiments (judge the dominant sentiment)\n",
      "\n",
      "\n",
      "# VALID VERDICTS\n",
      "You must choose one of the following verdicts:\n",
      "- Correct\n",
      "- Incorrect\n",
      "- Partially Correct\n",
      "\n",
      "# OUTPUT FORMAT\n",
      "You must provide your judgment in the following structured format:\n",
      "- score: A quality score from 0-100 evaluating the AI output\n",
      "- verdict: Your judgment (e.g., Correct, Incorrect, Partially Correct)\n",
      "- confidence: A score from 0-100 indicating your certainty in this judgment\n",
      "- reasoning: Your chain-of-thought explanation for why you gave this verdict\n",
      "- notes: Any additional observations or suggestions for improvement (optional)\n"
     ]
    }
   ],
   "source": [
    "# Create the LLM Judge\n",
    "sentiment_judge = LLMJudge(\n",
    "    llm_client=client,\n",
    "    role=judge_role,\n",
    "    task_description=evaluation_task,\n",
    "    evaluation_criteria=evaluation_criteria,\n",
    "    valid_verdicts=verdicts,\n",
    "    few_shot_examples=examples,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.3  # Low temperature for consistency\n",
    ")\n",
    "\n",
    "print(\"LLM Judge created successfully!\")\n",
    "print(f\"\\nSystem Prompt Preview (first 500 chars):\")\n",
    "print(sentiment_judge.get_system_prompt())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Judging Model Outputs\n",
    "\n",
    "Now let's use our judge to evaluate the model predictions from earlier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "JUDGING AI MODEL OUTPUTS\n",
      "====================================================================================================\n",
      "\n",
      "### Case 1 ###\n",
      "Input: This product is absolutely amazing! Best purchase ever!\n",
      "Model's Prediction: Positive\n",
      "Ground Truth (for reference): Positive\n",
      "^ Note: Ground truth shown here for education, but NOT passed to the judge!\n",
      "\n",
      "JUDGE VERDICT:       Correct\n",
      "Quality Score:       100.0/100\n",
      "Confidence:          100.0%\n",
      "\n",
      "Reasoning: The model correctly identified this as Positive sentiment. The review contains strong positive indicators such as 'absolutely amazing' and 'best purchase ever', which clearly express satisfaction and enthusiasm. The prediction is accurate and aligns perfectly with the sentiment expressed in the review.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "### Case 2 ###\n",
      "Input: The item arrived damaged and customer service was unhelpful.\n",
      "Model's Prediction: Negative\n",
      "Ground Truth (for reference): Negative\n",
      "^ Note: Ground truth shown here for education, but NOT passed to the judge!\n",
      "\n",
      "JUDGE VERDICT:       Correct\n",
      "Quality Score:       100.0/100\n",
      "Confidence:          95.0%\n",
      "\n",
      "Reasoning: The AI model correctly identified the sentiment as Negative. The review clearly expresses dissatisfaction with two strong indicators: 'arrived damaged' and 'customer service was unhelpful'. Both phrases indicate a negative experience, confirming the model's prediction is accurate and appropriate.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "### Case 3 ###\n",
      "Input: It's okay, nothing special but does the job I guess.\n",
      "Model's Prediction: Positive\n",
      "Ground Truth (for reference): Neutral\n",
      "^ Note: Ground truth shown here for education, but NOT passed to the judge!\n",
      "\n",
      "JUDGE VERDICT:       Incorrect\n",
      "Quality Score:       25.0/100\n",
      "Confidence:          90.0%\n",
      "\n",
      "Reasoning: The model incorrectly classified this review as Positive when it should be Neutral. The phrase 'It's okay' suggests a lukewarm sentiment, and 'nothing special' reinforces a lack of strong positive feelings. The statement 'does the job I guess' implies a sense of resignation rather than enthusiasm. Overall, the review does not express satisfaction or praise, making the Positive classification inaccurate.\n",
      "Notes: The model should improve its understanding of nuanced expressions of indifference or mild approval.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "### Case 4 ###\n",
      "Input: Terrible quality. Broke after one use. Very disappointed.\n",
      "Model's Prediction: Neutral\n",
      "Ground Truth (for reference): Negative\n",
      "^ Note: Ground truth shown here for education, but NOT passed to the judge!\n",
      "\n",
      "JUDGE VERDICT:       Incorrect\n",
      "Quality Score:       20.0/100\n",
      "Confidence:          90.0%\n",
      "\n",
      "Reasoning: The model incorrectly classified this review as Neutral when it should be Negative. The phrases 'terrible quality' and 'broke after one use' are strong indicators of dissatisfaction and negative sentiment. The expression 'very disappointed' further emphasizes the negative experience. Overall, the review conveys clear dissatisfaction, making the model's prediction a significant misclassification.\n",
      "Notes: The model should improve its ability to detect strong negative sentiment in phrases that clearly express disappointment.\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Judge each model prediction\n",
    "print(\"=\" * 100)\n",
    "print(\"JUDGING AI MODEL OUTPUTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, pred in enumerate(model_predictions, 1):\n",
    "    print(f\"\\n### Case {i} ###\")\n",
    "    print(f\"Input: {pred['input']}\")\n",
    "    print(f\"Model's Prediction: {pred['model_output']}\")\n",
    "    print(f\"Ground Truth (for reference): {pred['ground_truth']}\")\n",
    "    print(\"^ Note: Ground truth shown here for education, but NOT passed to the judge!\")\n",
    "    \n",
    "    # Judge the model's output - NO ground_truth parameter!\n",
    "    # The judge evaluates purely based on input + model_output\n",
    "    judgment = sentiment_judge.judge_single(\n",
    "        input_text=pred['input'],\n",
    "        model_output=pred['model_output']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'JUDGE VERDICT:':<20} {judgment.verdict}\")\n",
    "    print(f\"{'Quality Score:':<20} {judgment.score}/100\")\n",
    "    print(f\"{'Confidence:':<20} {judgment.confidence}%\")\n",
    "    print(f\"\\nReasoning: {judgment.reasoning}\")\n",
    "    if judgment.notes:\n",
    "        print(f\"Notes: {judgment.notes}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ReAct Pattern - Confidence and Reasoning\n",
    "\n",
    "The ReAct pattern (Reasoning + Acting) is built into our framework. Each judgment includes:\n",
    "\n",
    "1. **Score (0-100)**: Quantitative quality assessment\n",
    "2. **Verdict**: Binary or categorical judgment\n",
    "3. **Confidence**: How certain the judge is\n",
    "4. **Reasoning**: Chain-of-thought explanation\n",
    "5. **Notes**: Additional observations\n",
    "\n",
    "This enables:\n",
    "- **Transparency**: You can see why the judge made each decision\n",
    "- **Debugging**: Identify patterns in errors\n",
    "- **Human-in-the-loop**: Route low-confidence judgments to humans\n",
    "- **Quality control**: Track judge performance over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Batch Evaluation and Analytics\n",
    "\n",
    "Let's evaluate multiple predictions and analyze the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "                                                input model_output ground_truth   verdict  score  confidence\n",
      "This product is absolutely amazing! Best purchase ...     Positive     Positive   Correct  100.0       100.0\n",
      "The item arrived damaged and customer service was ...     Negative     Negative   Correct  100.0       100.0\n",
      "It's okay, nothing special but does the job I gues...     Positive      Neutral Incorrect   25.0        90.0\n",
      "Terrible quality. Broke after one use. Very disapp...      Neutral     Negative Incorrect   10.0        95.0\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS\n",
      "============================================================\n",
      "Total Predictions Evaluated: 4\n",
      "Correct Predictions: 2\n",
      "Incorrect Predictions: 2\n",
      "Partially Correct: 0\n",
      "\n",
      "Average Quality Score: 58.8/100\n",
      "Average Judge Confidence: 96.2%\n",
      "\n",
      "Model Accuracy (by judge): 50.0%\n"
     ]
    }
   ],
   "source": [
    "batch_input = [\n",
    "    (pred['input'], pred['model_output'])\n",
    "    for pred in model_predictions\n",
    "]\n",
    "\n",
    "# Batch judge\n",
    "judgments = sentiment_judge.judge_batch(batch_input)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'input': pred['input'][:50] + '...' if len(pred['input']) > 50 else pred['input'],\n",
    "        'model_output': pred['model_output'],\n",
    "        'ground_truth': pred['ground_truth'],\n",
    "        'verdict': j.verdict,\n",
    "        'score': j.score,\n",
    "        'confidence': j.confidence,\n",
    "    }\n",
    "    for pred, j in zip(model_predictions, judgments)\n",
    "])\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Calculate metrics\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"Total Predictions Evaluated: {len(judgments)}\")\n",
    "print(f\"Correct Predictions: {sum(1 for j in judgments if j.verdict == 'Correct')}\")\n",
    "print(f\"Incorrect Predictions: {sum(1 for j in judgments if j.verdict == 'Incorrect')}\")\n",
    "print(f\"Partially Correct: {sum(1 for j in judgments if j.verdict == 'Partially Correct')}\")\n",
    "print(f\"\\nAverage Quality Score: {results_df['score'].mean():.1f}/100\")\n",
    "print(f\"Average Judge Confidence: {results_df['confidence'].mean():.1f}%\")\n",
    "print(f\"\\nModel Accuracy (by judge): {sum(1 for j in judgments if j.verdict == 'Correct') / len(judgments) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>model_output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>verdict</th>\n",
       "      <th>score</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product is absolutely amazing! Best purch...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Correct</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The item arrived damaged and customer service ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Correct</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's okay, nothing special but does the job I ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>25.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Terrible quality. Broke after one use. Very di...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>10.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input model_output  \\\n",
       "0  This product is absolutely amazing! Best purch...     Positive   \n",
       "1  The item arrived damaged and customer service ...     Negative   \n",
       "2  It's okay, nothing special but does the job I ...     Positive   \n",
       "3  Terrible quality. Broke after one use. Very di...      Neutral   \n",
       "\n",
       "  ground_truth    verdict  score  confidence  \n",
       "0     Positive    Correct  100.0       100.0  \n",
       "1     Negative    Correct  100.0       100.0  \n",
       "2      Neutral  Incorrect   25.0        90.0  \n",
       "3     Negative  Incorrect   10.0        95.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Structured Output - Production Ready\n",
    "\n",
    "Our framework uses Pydantic for structured output, ensuring:\n",
    "- Type safety\n",
    "- Validation\n",
    "- Easy serialization\n",
    "- Clear schema\n",
    "\n",
    "Let's examine the structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JudgmentResult Schema:\n",
      "{'description': \"Structured output for judging an AI model's output.\\n\\nThis model enforces structured output from the LLM judge, ensuring:\\n- A quality score evaluating the AI output\\n- A binary verdict (correct/incorrect or pass/fail)\\n- A confidence score (0-100) in the judgment\\n- Chain-of-thought reasoning\\n- Optional notes for additional context\", 'properties': {'score': {'description': 'Quality score from 0-100 evaluating how good the AI output is', 'maximum': 100, 'minimum': 0, 'title': 'Score', 'type': 'number'}, 'verdict': {'description': \"Binary judgment: 'Correct', 'Incorrect', 'Partially Correct', 'Pass', 'Fail', etc.\", 'title': 'Verdict', 'type': 'string'}, 'confidence': {'description': \"Confidence score from 0 to 100 indicating the judge's certainty in this evaluation\", 'maximum': 100, 'minimum': 0, 'title': 'Confidence', 'type': 'number'}, 'reasoning': {'description': 'Chain-of-thought reasoning explaining why the AI output received this judgment', 'title': 'Reasoning', 'type': 'string'}, 'notes': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Additional observations, edge cases, or suggestions for improvement', 'title': 'Notes'}}, 'required': ['score', 'verdict', 'confidence', 'reasoning'], 'title': 'JudgmentResult', 'type': 'object'}\n",
      "\n",
      "Sample Judgment as JSON:\n",
      "{\n",
      "  \"score\": 100.0,\n",
      "  \"verdict\": \"Correct\",\n",
      "  \"confidence\": 100.0,\n",
      "  \"reasoning\": \"The model correctly identified this as Positive sentiment. The review contains strong positive indicators such as 'absolutely amazing' and 'best purchase ever', which clearly express satisfaction and praise for the product. The prediction is accurate and appropriate, reflecting the enthusiastic tone of the review.\",\n",
      "  \"notes\": null\n",
      "}\n",
      "\n",
      "Results saved to model_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Examine the Pydantic model\n",
    "print(\"JudgmentResult Schema:\")\n",
    "print(JudgmentResult.model_json_schema())\n",
    "\n",
    "# Serialize a judgment to JSON\n",
    "sample_judgment = judgments[0]\n",
    "print(\"\\nSample Judgment as JSON:\")\n",
    "print(sample_judgment.model_dump_json(indent=2))\n",
    "\n",
    "# Save all results to CSV\n",
    "full_results = []\n",
    "for pred, j in zip(model_predictions, judgments):\n",
    "    full_results.append({\n",
    "        'input': pred['input'],\n",
    "        'model_output': pred['model_output'],\n",
    "        'ground_truth': pred['ground_truth'],\n",
    "        'verdict': j.verdict,\n",
    "        'score': j.score,\n",
    "        'confidence': j.confidence,\n",
    "        'reasoning': j.reasoning,\n",
    "        'notes': j.notes\n",
    "    })\n",
    "\n",
    "results_df_full = pd.DataFrame(full_results)\n",
    "results_df_full.to_csv('model_evaluation_results.csv', index=False)\n",
    "print(\"\\nResults saved to model_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>model_output</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>verdict</th>\n",
       "      <th>score</th>\n",
       "      <th>confidence</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product is absolutely amazing! Best purch...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Correct</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>The model correctly identified this as Positiv...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The item arrived damaged and customer service ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Correct</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>The model correctly identified this as Negativ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's okay, nothing special but does the job I ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>25.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>The model incorrectly classified this review a...</td>\n",
       "      <td>The model should improve its ability to recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Terrible quality. Broke after one use. Very di...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>10.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>The model incorrectly classified this review a...</td>\n",
       "      <td>The model should improve its ability to detect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input model_output  \\\n",
       "0  This product is absolutely amazing! Best purch...     Positive   \n",
       "1  The item arrived damaged and customer service ...     Negative   \n",
       "2  It's okay, nothing special but does the job I ...     Positive   \n",
       "3  Terrible quality. Broke after one use. Very di...      Neutral   \n",
       "\n",
       "  ground_truth    verdict  score  confidence  \\\n",
       "0     Positive    Correct  100.0       100.0   \n",
       "1     Negative    Correct  100.0       100.0   \n",
       "2      Neutral  Incorrect   25.0        90.0   \n",
       "3     Negative  Incorrect   10.0        95.0   \n",
       "\n",
       "                                           reasoning  \\\n",
       "0  The model correctly identified this as Positiv...   \n",
       "1  The model correctly identified this as Negativ...   \n",
       "2  The model incorrectly classified this review a...   \n",
       "3  The model incorrectly classified this review a...   \n",
       "\n",
       "                                               notes  \n",
       "0                                               None  \n",
       "1                                                     \n",
       "2  The model should improve its ability to recogn...  \n",
       "3  The model should improve its ability to detect...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Real-World Application - Judging Without Ground Truth\n",
    "\n",
    "In many real-world scenarios, you don't have ground truth labels. The judge can still evaluate quality based on the criteria you define:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New predictions without ground truth\n",
    "new_predictions = [\n",
    "    {\n",
    "        \"input\": \"Love it! But wish it came in more colors.\",\n",
    "        \"model_output\": \"Positive\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Received it yesterday, setting it up now.\",\n",
    "        \"model_output\": \"Neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Not bad for the price, I guess.\",\n",
    "        \"model_output\": \"Positive\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Judging without ground truth:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, pred in enumerate(new_predictions, 1):\n",
    "    print(f\"\\n### Case {i} ###\")\n",
    "    print(f\"Input: {pred['input']}\")\n",
    "    print(f\"Model Output: {pred['model_output']}\")\n",
    "    \n",
    "    # Judge the model output - the judge evaluates quality without ground truth\n",
    "    judgment = sentiment_judge.judge_single(\n",
    "        input_text=pred['input'],\n",
    "        model_output=pred['model_output']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nVerdict: {judgment.verdict}\")\n",
    "    print(f\"Score: {judgment.score}/100\")\n",
    "    print(f\"Confidence: {judgment.confidence}%\")\n",
    "    print(f\"Reasoning: {judgment.reasoning}\")\n",
    "    print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Advanced - Judging Other Types of AI Outputs\n",
    "\n",
    "LLM-as-a-Judge isn't limited to classification. Let's see it evaluate chatbot responses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Chatbot Responses:\n",
      "====================================================================================================\n",
      "\n",
      "### Chatbot Exchange 1 ###\n",
      "Customer: What's your return policy?\n",
      "Chatbot: You can return items within 30 days for a full refund. Items must be unused and in original packaging.\n",
      "\n",
      "Verdict: Correct\n",
      "Score: 90.0/100\n",
      "Reasoning: The response accurately answers the customer's question about the return policy by specifying the time frame for returns and the condition of the items. It is clear, concise, and provides actionable information that the customer can follow. The tone is professional and appropriate for customer service.\n",
      "====================================================================================================\n",
      "\n",
      "### Chatbot Exchange 2 ###\n",
      "Customer: Is this product waterproof?\n",
      "Chatbot: This is a great product! Many customers love it!\n",
      "\n",
      "Verdict: Incorrect\n",
      "Score: 15.0/100\n",
      "Reasoning: The response does not answer the customer's question about whether the product is waterproof. Instead, it provides irrelevant information about customer satisfaction, which does not address the inquiry. The lack of a direct answer makes it unhelpful and inaccurate.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a judge for chatbot responses\n",
    "chatbot_role = \"\"\"You are an expert evaluator of customer service chatbot responses.\n",
    "You assess whether chatbot answers are helpful, accurate, and appropriately professional.\"\"\"\n",
    "\n",
    "chatbot_task = \"\"\"Evaluate whether the chatbot's response:\n",
    "1. Answers the customer's question\n",
    "2. Is accurate and helpful\n",
    "3. Maintains appropriate tone\n",
    "4. Provides actionable information\"\"\"\n",
    "\n",
    "chatbot_criteria = \"\"\"\n",
    "A good chatbot response should:\n",
    "- Directly address the customer's question\n",
    "- Be accurate (no hallucinations or false information)\n",
    "- Be concise but complete\n",
    "- Use professional, friendly tone\n",
    "- Provide next steps when appropriate\n",
    "\"\"\"\n",
    "\n",
    "chatbot_examples = [\n",
    "    FewShotExample(\n",
    "        input_text=\"How do I reset my password?\",\n",
    "        model_output=\"To reset your password, click 'Forgot Password' on the login page, enter your email, and follow the link sent to you.\",\n",
    "        expected_verdict=\"Correct\",\n",
    "        expected_score=95.0,\n",
    "        reasoning=\"The response directly answers the question with clear, actionable steps. It's concise, accurate, and helpful.\"\n",
    "    ),\n",
    "    FewShotExample(\n",
    "        input_text=\"When will my order arrive?\",\n",
    "        model_output=\"We offer free shipping on all orders!\",\n",
    "        expected_verdict=\"Incorrect\",\n",
    "        expected_score=10.0,\n",
    "        reasoning=\"The response completely ignores the question about delivery time. It provides irrelevant information about shipping cost instead of addressing the customer's concern.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "chatbot_judge = LLMJudge(\n",
    "    llm_client=client,\n",
    "    role=chatbot_role,\n",
    "    task_description=chatbot_task,\n",
    "    evaluation_criteria=chatbot_criteria,\n",
    "    valid_verdicts=[\"Correct\", \"Incorrect\", \"Partially Correct\"],\n",
    "    few_shot_examples=chatbot_examples,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Test chatbot responses\n",
    "chatbot_test = [\n",
    "    {\n",
    "        \"input\": \"What's your return policy?\",\n",
    "        \"output\": \"You can return items within 30 days for a full refund. Items must be unused and in original packaging.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Is this product waterproof?\",\n",
    "        \"output\": \"This is a great product! Many customers love it!\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Evaluating Chatbot Responses:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, test in enumerate(chatbot_test, 1):\n",
    "    print(f\"\\n### Chatbot Exchange {i} ###\")\n",
    "    print(f\"Customer: {test['input']}\")\n",
    "    print(f\"Chatbot: {test['output']}\")\n",
    "    \n",
    "    judgment = chatbot_judge.judge_single(\n",
    "        input_text=test['input'],\n",
    "        model_output=test['output']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nVerdict: {judgment.verdict}\")\n",
    "    print(f\"Score: {judgment.score}/100\")\n",
    "    print(f\"Reasoning: {judgment.reasoning}\")\n",
    "    print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Best Practices for LLM-as-a-Judge\n",
    "\n",
    "### 1. **Understand the Core Purpose**\n",
    "- LLM-as-a-Judge evaluates existing AI outputs, it doesn't create them\n",
    "- Use it for quality control, model evaluation, and human-in-the-loop workflows\n",
    "- It's not a replacement for the AI system, but a QA layer on top\n",
    "\n",
    "### 2. **Define Clear Evaluation Criteria**\n",
    "- Be specific about what makes an output \"good\" or \"bad\"\n",
    "- Provide concrete examples of correct and incorrect outputs\n",
    "- Consider edge cases and ambiguous situations\n",
    "\n",
    "### 3. **Use Comprehensive Few-Shot Examples**\n",
    "- Include examples of various verdict types (correct, incorrect, partial)\n",
    "- Show score calibration (what deserves 100 vs 60 vs 20)\n",
    "- Demonstrate the reasoning process\n",
    "- Cover edge cases your model struggles with\n",
    "\n",
    "### 4. **Leverage the ReAct Pattern**\n",
    "- Confidence scores help identify uncertain judgments\n",
    "- Reasoning provides transparency and debuggability\n",
    "- Notes can capture nuances and suggestions\n",
    "- This enables human-in-the-loop at scale\n",
    "\n",
    "### 5. **Use Structured Output**\n",
    "- Pydantic ensures type safety and validation\n",
    "- Makes integration with production systems easy\n",
    "- Enables easy analysis and aggregation\n",
    "- Provides clear schema documentation\n",
    "\n",
    "### 6. **Production Considerations**\n",
    "- **Cost**: Judge fewer samples or use cheaper models\n",
    "- **Speed**: Batch API for large evaluations\n",
    "- **Quality**: Use GPT-4 for critical judgments, GPT-3.5 for simpler ones\n",
    "- **Monitoring**: Track judge confidence and agreement with ground truth\n",
    "- **Iteration**: Continuously add new few-shot examples based on errors\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "1. **Model Evaluation**: Compare model versions without manual labeling\n",
    "2. **Quality Control**: Flag problematic AI outputs before they reach users\n",
    "3. **Active Learning**: Identify examples that need human review\n",
    "4. **Model Debugging**: Understand why/when your model fails\n",
    "5. **A/B Testing**: Evaluate which model variant performs better\n",
    "6. **Continuous Monitoring**: Track model quality in production\n",
    "\n",
    "### Going Further\n",
    "\n",
    "Extend this framework with:\n",
    "- **Multi-judge consensus**: Use multiple judges and aggregate verdicts\n",
    "- **Confidence-based routing**: Auto-approve high-confidence, review low-confidence\n",
    "- **Judge calibration**: Compare judge verdicts to ground truth\n",
    "- **Cost optimization**: Use smaller models for clearer cases\n",
    "- **Specialized judges**: Different judges for different evaluation aspects\n",
    "\n",
    "Happy judging! ðŸŽ¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Create your own judge\n",
    "\n",
    "# 1. Define what you're judging\n",
    "my_role = \"\"\"TODO: What kind of AI outputs are you evaluating?\"\"\"\n",
    "\n",
    "my_task = \"\"\"TODO: What should the judge assess?\"\"\"\n",
    "\n",
    "my_criteria = \"\"\"TODO: What makes an output good or bad?\"\"\"\n",
    "\n",
    "# 2. Create few-shot examples\n",
    "my_examples = [\n",
    "    # FewShotExample(\n",
    "    #     input_text=\"...\",\n",
    "    #     model_output=\"...\",\n",
    "    #     expected_verdict=\"Correct\",\n",
    "    #     expected_score=...,\n",
    "    #     reasoning=\"...\",\n",
    "    #     ground_truth=\"...\"  # optional\n",
    "    # ),\n",
    "]\n",
    "\n",
    "# 3. Create your judge\n",
    "# my_judge = LLMJudge(\n",
    "#     llm_client=client,\n",
    "#     role=my_role,\n",
    "#     task_description=my_task,\n",
    "#     evaluation_criteria=my_criteria,\n",
    "#     valid_verdicts=[\"Correct\", \"Incorrect\"],\n",
    "#     few_shot_examples=my_examples,\n",
    "#     model_name=\"gpt-4o-mini\",\n",
    "#     temperature=0.3\n",
    "# )\n",
    "\n",
    "# 4. Test it\n",
    "# judgment = my_judge.judge_single(\n",
    "#     input_text=\"your AI's input\",\n",
    "#     model_output=\"your AI's output\"\n",
    "# )\n",
    "# print(f\"Verdict: {judgment.verdict}\")\n",
    "# print(f\"Score: {judgment.score}\")\n",
    "# print(f\"Reasoning: {judgment.reasoning}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
